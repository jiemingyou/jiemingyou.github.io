[
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import groupby, combinations\nfrom copy import deepcopy\nfrom math import *\nimport random\nimport ast\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nsns.set_theme(style=\"white\")"
  },
  {
    "objectID": "prediction.html#labels",
    "href": "prediction.html#labels",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "Labels",
    "text": "Labels\nThe choice probabilities provided in the probabilities.txt file are \\(n\\)-dimensional arrays with \\(n\\) discrete choice options. In order to simplify our model and reduce the amount of model training required, we reduced the dimension of the label space by splitting each array (choice probability distribution) into \\(n\\) distinct labels, each consisting of a single probability. This way, instead of predicting an array of probabilities, we are only predicting the choice probability of a single product.\n\\[\n\\begin{aligned}\n\\text{original array} & & \\text{labels} \\\\\n\\left[\\textcolor{red}{0.4}, \\textcolor{OliveGreen}{0.1}, \\textcolor{blue}{0.5}\\right] & \\longrightarrow & [\\textcolor{red}{0.4}] \\\\\n&& [\\textcolor{OliveGreen}{0.1}] \\\\\n&&[\\textcolor{blue}{0.5}]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "prediction.html#features",
    "href": "prediction.html#features",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "Features",
    "text": "Features\nEach assortment array is also split into \\(n\\) feature vectors as follows; for each product in an assortment:\n\nSet the product as the first feature \\(x_1\\)\n\n\\[\n\\begin{aligned}\n\\text{original array} & & \\text{feature vectors} \\\\\n\\left[\\textcolor{red}{0}, \\textcolor{OliveGreen}{5}, \\textcolor{blue}{9}\\right] & \\longrightarrow & [\\textcolor{red}{0} \\\\\n&& [\\textcolor{OliveGreen}{5}\\\\\n&& [\\textcolor{blue}{9}\n\\end{aligned}\n\\]\n\nEncode the information of the other products in the assortment using multi-hot-encoding, also known as multi-label-binarizing\n\nBecause each assortment has varying amounts of products, the multi-hot-encoding is used to standardize the length of the feature vectors.\nAs there is an outside option and thirty (30) different products, the multi-label-binarizer has a shape of (1,31). For example, an assortment \\([0,5,9]\\) can be represented as\n\\[\n\\begin{aligned}\n\\text{Assortment} \\quad & [\\textcolor{red}{0}, \\textcolor{OliveGreen}{5}, \\textcolor{blue}{9}]\\\\\n\\text{Multi-hot} \\quad & [\\textcolor{red}{1},0,0,0,0,\\textcolor{OliveGreen}{1},0,0,0,\\textcolor{blue}{1},0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\\end{aligned}\n\\]\nHence, the final feature vectors are\n\\[\n\\begin{aligned}\n\\text{original array} &&  && \\text{feature vectors}\\\\\n[\\textcolor{red}{0}, \\textcolor{OliveGreen}{5}, \\textcolor{blue}{9}] && \\longrightarrow && [\\textcolor{red}{0},~ (0, 0, 0, 0, 0, \\textcolor{OliveGreen}{1}, 0, 0, 0, \\textcolor{blue}{1}, 0, ..., 0)]\\\\\n&&&& [\\textcolor{OliveGreen}{5}, ~(\\textcolor{red}{1}, 0, 0, 0, 0, 0, 0, 0, 0, \\textcolor{blue}{1}, 0, ..., 0)]\\\\\n&&&& [\\textcolor{blue}{9}, ~(\\textcolor{red}{1}, 0, 0, 0, 0, \\textcolor{OliveGreen}{1}, 0, 0, 0, 0, 0, ..., 0)]\n\\end{aligned}\n\\]\nAfter performing the transformations, we have a total of 8617 data points. The data points are finally split into separate training and testing sets. For the initial training, we will use the conventional 80-20 split and shuffle the datapoints to reduce the risk of overfitting.\n\nTEST_SIZE = 0.2    # Size of test set\nSHUFFLE = True     # Shuffle the datapoints\nRANDOM_STATE = 42  # For reproducibility"
  },
  {
    "objectID": "prediction.html#setting-up-the-data",
    "href": "prediction.html#setting-up-the-data",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "Setting up the data",
    "text": "Setting up the data\nFunctions used for the data transformation can be found at Appendix A.\nTransforming the data and splitting it into training and testing set\n\n# Splitting\nX_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n    assort, prob, test_size=TEST_SIZE, shuffle=SHUFFLE,\n    random_state=RANDOM_STATE\n)\n\n\n# Constructing the features and lables\nX_train, y_train = transform_data(X_train_raw, y_train_raw)\nX_test, y_test = transform_data(X_test_raw, y_test_raw)"
  },
  {
    "objectID": "prediction.html#model-evaluation",
    "href": "prediction.html#model-evaluation",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "Model evaluation",
    "text": "Model evaluation\nThe model’s predictive performance is assessed by calculating the mean-squared-error (MSE) of the predictions.\n\n# 1) Calculating the choice probabilities for the testing set\ny_preds = [ predict(assortment) for assortment in X_test_raw]\n    \n# 2) Calculating the mean MSE\nMSE_test = assortment_mse(y_preds, y_test_raw)\nprint(f\"The average MSE for an assortment is {np.mean(MSE_test) :.2f} %pp\")\nprint(f\"The median MSE for an assortment is {np.median(MSE_test) :.2f} %pp\")\n\nThe average MSE for an assortment is 1.46 %pp\nThe median MSE for an assortment is 0.51 %pp\n\n\n\n\n\n\n\nFigure 2: Assortments’ mean squared error (histogram). The source code can be found at Appendix C\n\n\n\n\nThe model performs well when tested on the testing set. The average MSE of the prediction for an assortment is under two percentage points. Furthermore, the median MSE is under one percentage point which indicates excellent prediction accuracy.\nAs seen from Figure 2, the long tail of the histogram is the cause of the average MSE being significantly worse than the median MSE.\nIn addition to the MSE, we can evaluate the popularity of the products by calculating the feature importance for the multi-hot encoded array. If a feature is important, its value has a significant effect on the prediction. In other words, the products with the highest corresponding importance are the most sought after as their presence in the assortment have a signicant impact on the choice probability of the other product.\nThe feature importance of the produts are shown in Figure 3. Based on the observations, the products \\(a\\), \\(e\\), and \\(f\\) are less popular, whereas the products \\(b\\), \\(c\\), and \\(d\\) are more popular.\n\nfeat_imp = pd.Series(reg.feature_importances_[2:])\nfeat_imp.set_axis(range(1,31), inplace=True)\nfeat_imp.plot(kind='barh')\nplt.show()\n\n\n\n\nFigure 3: Feature importances of the products. The outside option (\\(x_2^{(i)}\\)) is omitted."
  },
  {
    "objectID": "pricing.html",
    "href": "pricing.html",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import groupby, combinations\nfrom copy import deepcopy\nfrom math import *\nimport random\nimport ast\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nsns.set_theme(style=\"white\")"
  },
  {
    "objectID": "pricing.html#labels",
    "href": "pricing.html#labels",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "Labels",
    "text": "Labels\nThe choice probabilities provided in the probabilities.txt file are \\(n\\)-dimensional arrays with \\(n\\) discrete choice options. In order to simplify our model and reduce the amount of model training required, we reduced the dimension of the label space by splitting each array (choice probability distribution) into \\(n\\) distinct labels, each consisting of a single probability. This way, instead of predicting an array of probabilities, we are only predicting the choice probability of a single product.\n\\[\n\\begin{aligned}\n\\text{original array} & & \\text{labels} \\\\\n\\left[\\textcolor{red}{0.4}, \\textcolor{OliveGreen}{0.1}, \\textcolor{blue}{0.5}\\right] & \\longrightarrow & [\\textcolor{red}{0.4}] \\\\\n&& [\\textcolor{OliveGreen}{0.1}] \\\\\n&&[\\textcolor{blue}{0.5}]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pricing.html#features",
    "href": "pricing.html#features",
    "title": "Choice Prediction and Assortment Optimization using Random Forest Regressor",
    "section": "Features",
    "text": "Features\nEach assortment array is also split into \\(n\\) feature vectors as follows; for each product in an assortment:\n\nSet the product as the first feature \\(x_1\\)\n\n\\[\n\\begin{aligned}\n\\text{original array} & & \\text{feature vectors} \\\\\n\\left[\\textcolor{red}{0}, \\textcolor{OliveGreen}{5}, \\textcolor{blue}{9}\\right] & \\longrightarrow & [\\textcolor{red}{0} \\\\\n&& [\\textcolor{OliveGreen}{5}\\\\\n&& [\\textcolor{blue}{9}\n\\end{aligned}\n\\]\n\nEncode the information of the other products in the assortment using multi-hot-encoding, also known as multi-label-binarizing\n\nBecause each assortment has varying amounts of products, the multi-hot-encoding is used to standardize the length of the feature vectors.\nAs there is an outside option and thirty (30) different products, the multi-label-binarizer has a shape of (1,31). For example, an assortment \\([0,5,9]\\) can be represented as\n\\[\n\\begin{aligned}\n\\text{Assortment} \\quad & [\\textcolor{red}{0}, \\textcolor{OliveGreen}{5}, \\textcolor{blue}{9}]\\\\\n\\text{Multi-hot} \\quad & [\\textcolor{red}{1},0,0,0,0,\\textcolor{OliveGreen}{1},0,0,0,\\textcolor{blue}{1},0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\\end{aligned}\n\\]\nHence, the final feature vectors are\n\\[\n\\begin{aligned}\n\\text{original array} &&  && \\text{feature vectors}\\\\\n[\\textcolor{red}{0}, \\textcolor{OliveGreen}{5}, \\textcolor{blue}{9}] && \\longrightarrow && [\\textcolor{red}{0},~ (0, 0, 0, 0, 0, \\textcolor{OliveGreen}{1}, 0, 0, 0, \\textcolor{blue}{1}, 0, ..., 0)]\\\\\n&&&& [\\textcolor{OliveGreen}{5}, ~(\\textcolor{red}{1}, 0, 0, 0, 0, 0, 0, 0, 0, \\textcolor{blue}{1}, 0, ..., 0)]\\\\\n&&&& [\\textcolor{blue}{9}, ~(\\textcolor{red}{1}, 0, 0, 0, 0, \\textcolor{OliveGreen}{1}, 0, 0, 0, 0, 0, ..., 0)]\n\\end{aligned}\n\\]\nAfter performing the transformations, we have a total of 8617 data points. The data points are finally split into separate training and testing sets. For the initial training, we will use the conventional 80-20 split and shuffle the datapoints to reduce the risk of overfitting."
  },
  {
    "objectID": "convolution.html",
    "href": "convolution.html",
    "title": "Kernel convolutions",
    "section": "",
    "text": "Applying convolution matrices to rasters. Convolutions and matrices calculated using base R.\n\nlibrary(jpeg)\nset.seed(123)\n\n\nLoading the image\nReading the image from a jpg file into a raster array. The image is loaded into an array of (500, 500, 3).\n\n# Reading the sample image\nimg <- readJPEG(\"assets/wall.jpg\", native = FALSE)\ndim(img)\n\n[1] 500 500   3\n\n\nPlotting the the raster image.\n\n\n\nplotting function\n\nplot_raster <- function(raster) {\n  par(mfrow = c(1,1), mar = c(1,1,1,1))\n  plot.new()\n  as.raster(raster) |>\n    rasterImage(xleft = 0, xright = 1, ytop = 0, ybottom = 1)\n}\n\n\n\nplot_raster(img)\n\n\n\n\nOriginal photo\n\n\n\n\n\n\nConvolving kernel\nThe convolution can be expressed as \\[\ng(x,y) = M \\cdot f(x,y) = \\sum_{dx=-a}^{a} \\sum_{dy=-b}^{b} M(dx, dy) f(x-dx, y-dy)\n\\]\nThat is, we calculate a new value for each pixel using the product of the convolution kernel and matrix of the same dimension from array f, in a rolling basis.\nHowever, the edge pixels might cause problems when using a large kernel, as there are no adjacent pixels. This can be handled by edge padding.\n\nExtended edge padding\nIn order to calculate convolutions also for the edge pixels, we can extend the edges by repeating edge values indefinitely.\n\n\n\n\n\nPadding edge pixels by extension (Michael 2013).\n\n\n\n\nThe following function repeats the edge pixels of a given matrix for a specified mount of times.\n\nedge_extension <- function(mat, pad) {\n    n <- nrow(mat)\n    m <- ncol(mat)\n\n    top   <- c(rep(mat[1,1], pad), mat[1,], rep(mat[1,m], pad))\n    bot   <- c(rep(mat[n,1], pad), mat[n,], rep(mat[n,m], pad))\n    left  <- matrix(rep(mat[1:n, 1], pad), ncol=pad)\n    right <- matrix(rep(mat[1:n, m], pad), ncol=pad)\n    mid   <- cbind(left, mat[1:n,], right)\n\n    new_mat <- rbind(\n        matrix(rep(top, pad), nrow=pad, byrow=TRUE),\n        mid,\n        matrix(rep(bot, pad), nrow=pad, byrow=TRUE)\n    )\n    \n    return(new_mat)\n}\n\nThe function in action when given a following 3x3 matrix as input \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\]\n\nedge_extension(matrix(1:9, ncol=3, byrow=T), pad=2) |>\n  write.table(row.names=F, col.names=F)\n\n1 1 1 2 3 3 3\n1 1 1 2 3 3 3\n1 1 1 2 3 3 3\n4 4 4 5 6 6 6\n7 7 7 8 9 9 9\n7 7 7 8 9 9 9\n7 7 7 8 9 9 9\n\n\n\n\nApplying the function\nThe following function calculates the rolling kernel using edge extension\n\nrolling_kernel <- function(f, M) {\n  \n  x <- ncol(f)\n  y <- nrow(f)\n  m <- ncol(M)\n  n <- nrow(M)\n  g <- f # g(x,y)\n  \n  # Kernel dimensions exceed the source\n  if (m > x ||  n  > y) {\n    stop(\"Kernel length exceeds the source\")\n  }\n  \n  g_extended <- edge_extension(f, (m-1))\n  \n  for (i in 1:x) {\n    for (j in 1:y) {\n      f_kernel <- g_extended[i:(i+(m-1)), j:(j+(n-1))]\n      g_kernel <- f_kernel * M\n      g[i, j] <- sum(g_kernel)\n    }\n  }\n\n  return(g)\n}\n\nA main wrapper function that convolves the given kernel for all three channels of an RGB-image. We have to also scale the values into [0,1] by computing \\(\\frac{\\mathrm{rank}(A_{m \\times n})}{m \\cdot n}\\).\n\nrgb_kernel <- function(raster, kernel) {\n  channels <- dim(raster)[3]\n  output <- raster\n\n  for (ch in 1:channels) {\n    output[,,ch] <- rolling_kernel(raster[,,ch], kernel)\n  }\n\n  # Scaling\n  output_reshape <- matrix(output, nrow = ncol(raster))\n  output_scaled  <- rank(output_reshape) / length(output_reshape)\n  dim(output_scaled) <- c(dim(raster))\n  \n  return(output_scaled)\n}\n\n\n\nTesting out common kernels\n\n# Sharpen\nimg |>\n  rgb_kernel(matrix(c(0,-1,0,-1,5,-1,0,-1,0), nrow=3)) |>\n  plot_raster()\n\n\n\n# Edge detection  \nimg |>\n  rgb_kernel(matrix(c(-1,-1,-1,-1,8,-1,-1,-1,-1), nrow=3)) |>\n  plot_raster()\n\n\n\n# Box blur\nimg |>\n  rgb_kernel(matrix(rep(1, 81), nrow=9)) |>\n  plot_raster()\n\n\n\n\n\n\n\n2-D Gaussian kernel\nGaussian blur can be applied by running a convolution using a Gaussian kernel. The two-dimensional Guaissian kernel is defined as \\[\nG(x,y) = \\frac{1}{2 \\pi \\sigma ^2} e^{- \\frac{x^2 + y^2}{2 \\sigma ^2}}\n\\]\nUsing the CDF of normal distribution, we can calculate the convolution matrix by\n\nGenerating a linspace \\(u = \\texttt{linspace}[\\texttt{a}, \\texttt{b}] \\in \\mathbb{R}^n\\)\nUsing \\(u\\) to draw values from the Gaussian \\(\\texttt{CDF}\\)\nCalculating the convolution matrix \\(M\\) by taking the outer product \\(M = uu^\\top \\in \\mathbb{R}^{n \\times n}\\)\n\n\n# Gaussian kernel function\ngaussian_kernel <- function(width, sigma = 1) {\n    if (width%%2 != 1) { stop(\"Even width parameter\") }\n\n    # Boundaries\n    b  <- (width-1)/2\n    a <- -b\n    \n    # Setting up the linspace\n    ax <- seq(a, b, length = width)\n\n    # Draw from gaussian CDF\n    u <- dnorm(ax, sd = sigma)\n\n    # Outer product uu^T and normalization\n    M <- u %o% u\n    M <- M / sum(M)\n\n    return(M)\n}\n\nPlotting an example convolution matrix using width = 81 and sigma = 10.\n\n\nCalculating color values for the plot\n# Modified from: https://stackoverflow.com/a/39118422\nz <- gaussian_kernel(width = 81, sigma = 10)\ncolors <- colorRampPalette(c(\"green\", \"yellow\", \"red\"))(100)\n\nz.facet.range <- ((z[-1, -1] +\n                   z[-1, -ncol(z)] +\n                   z[-nrow(z), -1] +\n                   z[-nrow(z), -ncol(z)]) / 4) |>\n                \n                cut(100)\n\n\n\n# Preparing the variables\nM <- gaussian_kernel(width = 81, sigma = 10)\nx <- y <- seq(-40, 40, length = 81)\n\n# Plotting the 3D surface\npar(mfrow = c(1,1), mar = c(1,1,1,1))\npersp(x, y, M,\n    theta = 135, phi = 30,\n    zlab = \"density\",\n    col = colors[z.facet.range]\n)\n\n\n\n\n3-D representation of the 2-D gaussian convolution\n\n\n\n\n\n\nGaussian blurring\nFinally, convolving the gaussian kernel and applying a Gaussian Blurof radius: 40 and sd: 10.\n\n# Gaussian blur\nimg |>\n  rgb_kernel(gaussian_kernel(width = 81, sigma = 10)) |>\n  plot_raster()\n\n\n\n\nOriginal image + gaussian blur\n\n\n\n\n\n\nReferences\n\n\nMichael, Plotke. 2013. Image Kernel Convolution, Extend Edge-Handling. Wikipedia. https://en.wikipedia.org/wiki/File:Extend_Edge-Handling.png."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jieming You",
    "section": "",
    "text": "I’m Jieming You, an undergraduate IEM (Industrial Engineering and Management) student at Aalto University.\nThis website is for archiving and documenting my personal projects mainly related to programming. You can find my photography work here."
  },
  {
    "objectID": "sanajahti.html",
    "href": "sanajahti.html",
    "title": "Sanajahti Solver",
    "section": "",
    "text": "Site work in progress…"
  },
  {
    "objectID": "bayesian/bayesian.html",
    "href": "bayesian/bayesian.html",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "",
    "text": "There has been, and still is, a lot of controversy about the presence and magnitude of climate change. According to many studies, earth’s temperature has been steadily rising due to different factors, such as increased global emissions and deforestation. These studies propose differing values for the rate of the warming. Based on a report of IPCC, human-induced warming is likely increasing the average temperatures between 0.1°C and 0.3°C per decade.\nThe main analysis problem of this project is to find out the presence and the rate of global warming. We aim to model the global warming rate using Bayesian workflow. The analysis will be carried out using temperature observations gathered from 4 cities (Beijing, Helsinki, Los Angeles, Sydney) from a 25-year time span. We will fit three variants of the Gaussian linear model to average monthly temperatures of the four different locations. The goal is then to find an estimate for the slope of the linear model. As an example, the monthly temperature fluctuation of the target cities are illustrated in Figure 1.\n\n\n\n\n\nFigure 1: Monthly mean temperature"
  },
  {
    "objectID": "bayesian/bayesian.html#data",
    "href": "bayesian/bayesian.html#data",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\nidx\ntemp\nlocation\n\n\n\n\n1\n13.38889\nlos_angeles\n\n\n2\n15.74008\nlos_angeles\n\n\n3\n15.31541\nlos_angeles\n\n\n4\n15.32037\nlos_angeles\n\n\n5\n15.14695\nlos_angeles\n\n\n\n\nThe data is gathered from the Average Daily Temperature Archive of the University of Dayton which originates from the National Climatic Data Center. The extracted features were month, year and temperature (F). For each city, we have 9250 records covering every month from 1.1.1995 to 13.5.2020, having a data set of 37000 rows.\nWe have done modest data preparation mainly for the temperature. Firstly, we converted the Fahrenheit temperatures to Celsius. Secondly, null values were omitted from the dataset. The daily temperature observations were aggregated into monthly averages which were then re-indexed by turning into month-indices (1 to 305)."
  },
  {
    "objectID": "bayesian/bayesian.html#models",
    "href": "bayesian/bayesian.html#models",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Models",
    "text": "Models\nWe have chosen to use three variants of the linear Gaussian model for this analysis. This means that we assume the change in the average temperature to be linear. Even if the actual dependency is nonlinear, the linear model should be able to capture the overall trend of the temperatures. In addition, the time span of our data (25 years) is too narrow to reliably fit more sophisticated models accurately.\nIn our first model, the separate model (Equation 1), we treat each of the four measurement locations separately, i.e., we assume there to be no connections between the locations. A linear Gaussian model is fitted separately to the data of each measurement location.\n\\[\ny_i \\sim N(\\alpha_i + \\beta_i x_i,~\\sigma_i)\n\\tag{1}\\]\nIn the pooled model (Equation 2), we assume there to be no distinction between the measurement locations. In the model all data is gathered together, and a single linear model is fitted to the data. With the combined data, we expect to see higher variances and larger posterior intervals due to the different natures of the measurement locations.\n\\[\ny \\sim N(\\alpha + \\beta x,~\\sigma)\n\\tag{2}\\]\nIn the hierarchical linear model (Equation 3), we assume that slopes are drawn from a common hyper prior distribution. The hierarchical model assumes that the possible increases in the average temperature correlates between locations. Or put simply: If there is warming, the model assumes it to be a global phenomenon. The other parameters, intercept and standard deviation, are assumed to be individual to the measurement location since they represent the annual average (at the intercept) and the seasonal fluctuation of the temperature.\n\\[\n\\begin{aligned}\ny_i \\sim  &N(\\alpha_i + \\beta_i x_i,~\\sigma_i) \\\\\n\\beta_i \\sim &N(\\mu_0, \\sigma_0)\n\\end{aligned}\n\\tag{3}\\]"
  },
  {
    "objectID": "bayesian/bayesian.html#priors",
    "href": "bayesian/bayesian.html#priors",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Priors",
    "text": "Priors\nWe chose to use weakly informative normal priors for all the models to avoid any biases that more informative priors may introduce. In particular, we wanted a very weakly informative prior for the slope parameter to maximize the influence of the data on the outcome.\nIn our model, the slope parameter is of the form degrees per month, which means that the values for the slope are expected to be quite close to \\(0\\). The prior for the slope was chosen to be \\(N(0, 0.1)\\). The mean was chosen to be \\(0\\) to avoid any biases – it is equally likely for the slope to be positive or negative. Slope of \\(0.1 °C\\) corresponds to over \\(1\\) degree increase in the average for each year. The rate of \\(1\\) degree per year is very unlikely and thus the prior should be very weakly informative.\n\\[\n\\begin{aligned}\n\\beta &\\sim N(0, 0.1) \\\\\n\\beta_i &\\sim N(0, 0.1)\n\\end{aligned}\n\\]\nThe prior for the intercept, \\(N(10, 20)\\), is loosely based on available information of the average temperature on earth (\\(14°C\\) with oceans, \\(8°C\\) land only). The standard deviation was intentionally set quite high to keep the prior weak. In our model the intercept is in the beginning of the year 1995, which means that the intercept represents the annual average temperature close to that time.\n\\[\n\\begin{aligned}\n\\alpha &\\sim N(10, 20) \\\\\n\\alpha_i &\\sim N(10, 20)\n\\end{aligned}\n\\]\nFor the standard deviation in the linear model, \\(N(0, 20)\\) was used. The standard deviation corresponds to the annual fluctuations in the monthly averages. The prior covers a 40 degree within 1 std range and hence it should be weakly informative even in the locations with high fluctuations in the monthly averages, such as Helsinki.\n\\[\n\\begin{aligned}\n\\sigma &\\sim N(0, 20) \\\\\n\\sigma_i &\\sim N(0, 20)\n\\end{aligned}\n\\]\nIn the hierarchical model the hyper priors for the mean and standard deviation of the slope were chosen to be \\(N(0, 0.1)\\) based on the prior used in the separate model.\n\\[\n\\begin{aligned}\n\\mu_0 \\sim & N(0, 0.1) \\\\\n\\sigma_0 \\sim & N(0, 0.1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "bayesian/bayesian.html#mcmc-sampling",
    "href": "bayesian/bayesian.html#mcmc-sampling",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "MCMC sampling",
    "text": "MCMC sampling\nWe used RStan to perform the Markov Chain Monte Carlo simulation. Three Stan models for separate, pooled, and hierarchical model was used, respectively. The models use hard-coded priors, which were manually altered during sensitivity analysis.\nHierarchical model\n\n\nShow the code\n// Separate Gaussian linear model\ndata {\n  int<lower=0> N;                   // number of data points\n  int<lower=1> J;                   // number of groups\n  vector[N] x;                      // dates\n  vector[N] y;                      // temperatures\n  array[N] int<lower=1, upper=J> g; // group indicators\n}\n\nparameters {\n  // Separate parameters for all groups\n  vector[J] alpha;           // intercept\n  vector[J] beta;            // slope\n  vector<lower=0>[J] sigma;  // std \n}\n\ntransformed parameters {\n  // Linear model using separate intercept and slope for all sites\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = alpha[g[i]] + beta[g[i]] * x[i];\n  }\n}\n\nmodel {\n  alpha ~ normal(10, 20); // Normal prior for the intercept\n  beta  ~ normal(0, 0.1); // Normal prior for slope\n  sigma ~ normal(0, 20);  // Half normal prior for Std\n  \n  // Observation model using the separate parameters\n  for (i in 1:N) {\n    y[i] ~ normal(mu[i], sigma[g[i]]);\n  }\n}\n\ngenerated quantities {\n  array[N] real log_lik;\n  array[N] real y_rep;\n  \n  for (i in 1:N) {\n    y_rep[i] = normal_rng(mu[i], sigma[g[i]]);\n    log_lik[i] = normal_lpdf(y[i] | mu[i], sigma[g[i]]);\n  }\n}\n\n\nPooled model\n\n\nShow the code\n// Pooled Gaussian linear model\ndata {\n  int<lower=0> N;       // number of data points\n  vector[N] x;          // dates\n  vector[N] y;          // temperatures\n  \n}\n\nparameters {\n  // Common parameters\n  real alpha;           // intercept\n  real beta;            // slope\n  real<lower=0> sigma;  // std \n}\n\ntransformed parameters {\n  // Linear model \n  vector[N] mu = alpha + beta*x;\n}\n\nmodel {\n  alpha ~ normal(10, 20); // Normal prior for intercept\n  beta  ~ normal(0, 0.1); // Normal prior for slope\n  sigma ~ normal(0, 20);  // Half normal prior for Std\n  \n  // Observation model\n  y ~ normal(mu, sigma);\n}\n\ngenerated quantities {\n  array[N] real log_lik;\n  array[N] real y_rep;\n  \n  for (i in 1:N) {\n    y_rep[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(y[i] | mu[i], sigma);\n  }\n}\n\n\nHierarchical model\n\n\nShow the code\n// Hierarchical Gaussian linear model\ndata {\n  int<lower=0> N;                   // number of data points\n  int<lower=1> J;                   // number of groups\n  vector[N] x;                      // dates\n  vector[N] y;                      // temperatures\n  array[N] int<lower=1, upper=J> g; // group indicators\n\n  \n}\n\nparameters {\n  // Separate intercept and std. Hierarchical slope\n  vector[J] alpha;           // intercept\n  vector[J] beta;            // slope\n  vector<lower=0>[J] sigma;  // std \n  // Params for beta\n  real mu0;                  // mean\n  real<lower=0> sigma0;      // std\n  \n}\n\ntransformed parameters {\n  // Linear model using separate intercepts and hierarchical slopes\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = alpha[g[i]] + beta[g[i]] * x[i];\n  }\n}\n\nmodel {\n  mu0 ~ normal(-1, 0.1);        // Hyper priors\n  sigma0 ~ normal(-1, 0.1);\n  \n  alpha ~ normal(10, 20);      // Normal prior for the intercept\n  beta  ~ normal(mu0, sigma0); // Normal prior for slope\n  sigma ~ normal(0, 20);       // Half normal prior for Std\n  \n  // Observation model\n  for (i in 1:N) {\n    y[i] ~ normal(mu[i], sigma[g[i]]);\n  }\n}\n\ngenerated quantities {\n  array[N] real log_lik;\n  array[N] real y_rep;\n  real beta_pred;\n  \n  beta_pred = normal_rng(mu0, sigma0);\n  \n  for (i in 1:N) {\n    y_rep[i] = normal_rng(mu[i], sigma[g[i]]);\n    \n    log_lik[i] = normal_lpdf(y[i] | mu[i], sigma[g[i]]);\n  }\n}\n\n\nThe Stan models were run with 4 chains and 500 + 500 iterations per chain. For the hierarchical model, the iterations were raised to 1000 + 1000 to reduce \\(\\hat{R}\\) values.\n\ndata <- list(\n  x = temp_data$idx,    # month idx\n  y = temp_data$temp,   # temps\n  g = temp_data$group,  # group indicators\n  N = nrow(temp_data),  # number of data points\n  J = length(cities)    # number of groups\n)\n\nfit_sep  <- separate$sample(data=data)\nfit_pool <- pooled$sample(data=data)\nfit_hier <- hier$sample(data=data, step_size = 0.1,\n                        adapt_delta=.95)\n\nWith the hierarchical model, approximately 5% to 15% of transitions ended in divergence during warm-up. This was probably due to the scale parameter, standard deviation, beta getting zero values as a result of the narrow half-normal hyper prior. This was later addressed by setting a custom adapt_delta parameter for the sampling."
  },
  {
    "objectID": "bayesian/bayesian.html#convergence-diagnostics",
    "href": "bayesian/bayesian.html#convergence-diagnostics",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Convergence diagnostics",
    "text": "Convergence diagnostics\nSince the separate and hierarchical have different variables for each city, we only check the convergence for one city (Beijing). Pooled model on the other hand, has naturally one alpha and beta.\n\nSeparate model\n\n\n\n\n\n\n\n\n\nvariable\nrhat\ness_bulk\n\n\n\n\nalpha[4]\n0.9998801\n2751.634\n\n\nbeta[4]\n1.0001953\n2997.470\n\n\nsigma[4]\n1.0020886\n4373.331\n\n\n\n\nFor the separate model, the R values for the slope and intercept are below the commonly used threshold 1.01. Hence, the chains are converged and the results can be considered reliable. The effective sample sizes (ess_bulk) are high, which means that there is little correlation between the samples, i.e., the model should capture the whole distribution. No problems with divergences or tree depth were encountered.\n\n\nPooled model\n\n\n\n\n\n\nvariable\nrhat\ness_bulk\n\n\n\n\nalpha\n1.0002801\n1880.207\n\n\nbeta\n0.9999699\n1983.445\n\n\nsigma\n1.0030777\n1981.519\n\n\n\n\nFor the pooled model the \\(\\hat{R}\\) values are slightly higher than with the separate model but they are less than 1.01. Hence, we can confidently assume that the chains have converged. The effective sample sizes ess_bulk are about half of the sizes from separate model. This is expected as the we have “pooled” all the data in to a single group. The values are still high enough and should not cause any problems. No problems with divergences or tree depth were encountered.\n\n\nHierarchical model\n\n\n\n\n\n\nvariable\nrhat\ness_bulk\n\n\n\n\nalpha[4]\n1.003701\n1942.1847\n\n\nbeta[4]\n1.006525\n2129.3775\n\n\nsigma[4]\n1.001826\n2967.4605\n\n\nmu0\n1.002452\n1821.2931\n\n\nsigma0\n1.008439\n442.9684\n\n\n\n\nFor the hierarchical model, the \\(\\hat{R}\\) values for the slope and intercept are slightly above the commonly used threshold 1.01. In addition, \\(\\hat{R}\\) values for the hyper prior parameters are above the threshold especially for sigma0. The \\(\\widehat{ESS}\\) of sigma0 is also considerably smaller than \\(\\widehat{ESS}\\) of other parameters. This is probably due to divergent transitions during warm-up. We increased the adapt_delta and the number of iterations from the initial to 500 + 500 to reduce the \\(\\hat{R}\\) values.\nAll divergent transitions happened during warm-up, which is not as serious as divergences after warmup. However, the percentage of the divergent transitions is quite high, which may negatively affect the convergence and thus reliability."
  },
  {
    "objectID": "bayesian/bayesian.html#posterior-predictive-checks",
    "href": "bayesian/bayesian.html#posterior-predictive-checks",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Posterior predictive checks",
    "text": "Posterior predictive checks\nPosterior predictivity was tested by sampling replicated values from the posterior distribution and comparing them to the empirical data. As can be seen from Figure 2, Figure 3, and Figure 4 the model can not capture the true nature of the data due to the annual fluctuation in the monthly average temperature. However, in our case we are mainly interested in the annual mean and its increase, i.e., the discrepancies between the replicated and actual data should not be an issue.\n\n\n\n\n\n\n\n\nFigure 2: Separate model posterior draws\n\n\n\n\n\n\n\n\n\nFigure 3: Pooled model posterior draws\n\n\n\n\n\n\n\n\n\nFigure 4: Hierarchical model posterior draws"
  },
  {
    "objectID": "bayesian/bayesian.html#model-comparison",
    "href": "bayesian/bayesian.html#model-comparison",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Model Comparison",
    "text": "Model Comparison\nThe models are compared using PSIS-LOO values and the reliability of the estimates is assessed using pareto-k values.\n\n\n\n\n\n\nPareto k-values \n\n\nstat\nseparate\npooled\nhierarchical\n\n\n\n\n1st Qu.\n-0.077\n-0.148\n-0.098\n\n\n3rd Qu.\n0.021\n-0.067\n0.016\n\n\nMax.\n0.277\n0.097\n0.235\n\n\nMean\n-0.029\n-0.106\n-0.041\n\n\nMedian\n-0.025\n-0.120\n-0.032\n\n\nMin.\n-0.382\n-0.247\n-0.332\n\n\n\n\n\n\n\n\nRounding the elpd_loo based on the MCSE of 0.1 \n\n\nloo_separate_model\nloo_pooled_model\nloo_hierarchical_model\n\n\n\n\n-3828\n-4317\n-3826\n\n\n\n\n\nAll models have k-values less than 0.5. Hence, the obtained elpd values can be considered reliable. Based on the obtained values, the hierarchical model had the greatest elpd-value (-3826) and it is the most suitable model. However, due to the convergence issues with the hierarchical model the separate model can be considered a good alternative."
  },
  {
    "objectID": "bayesian/bayesian.html#prior-sensitivity",
    "href": "bayesian/bayesian.html#prior-sensitivity",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Prior Sensitivity",
    "text": "Prior Sensitivity\nWhen using sensible priors, the model is not greatly affected by the prior choice as there are sufficient data points to reduce the prior’s effect on the posterior model.\n\n\n\nBeta prior sensitivity to posterior distributions"
  },
  {
    "objectID": "bayesian/bayesian.html#discussion",
    "href": "bayesian/bayesian.html#discussion",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Discussion",
    "text": "Discussion\nDuring the analysis process we noticed that using monthly averages seems to induce a lot of variance to the results due to the annual fluctuation in temperature. This can be observed by comparing the confidence intervals from Sydney and Los Angeles (less fluctuation) to the intervals of Helsinki and Beijing (more fluctuation). Tighter intervals were obtained for the slope for cities with less annual fluctuation.\nThe hierarchical model produced larger PSIS-LOO values than the separate model. However, due to the divergences and \\(\\hat{R}\\) issues, the separate model should also be considered as a viable option. The Stan code of the hierarchical model should be revised to reduce the number of divergent transitions during warm-up.\nPossible improvements for the current model include using data from a wider time span and using other metrics instead of the monthly averages, which induce a lot of uncertainty to the posterior distributions."
  },
  {
    "objectID": "bayesian/bayesian.html#conclusion",
    "href": "bayesian/bayesian.html#conclusion",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "Conclusion",
    "text": "Conclusion\nWith all models used in this report the predicted mean of the slope parameter was positive and on the order of 0.4 degrees per decade. The 80% credible intervals are quite wide, however, even the lower bound is on the positive side with most cities. Beijing has the widest interval regardless of the used model and it includes also negative slope values. Figure 5 presents the posterior distributions of the slope parameter for the separate model.\n\n\n\n\n\nFigure 5: Predictive distribution sampled using hierarchical hyperpriors µ_0 and σ_0\n\n\n\n\nThe used priors do not significantly affect the results. Thus, we can conclude that the results should not be biased in any way. The used models produce results that agree with other research done on the topic. The observed global warming rate is around 0.1-0.3 degrees per decade (IPCC, n.d.). Our model’s 80% posterior interval for the slope was around 0.02-0.08 degrees per year or around 0.2-0.8 degrees per decade (Figure 5). However, the results of this report cannot be considered accurate nor reliable. More sophisticated analysis is needed to obtain more accurate estimates for the rate of warming."
  },
  {
    "objectID": "bayesian/bayesian.html#references",
    "href": "bayesian/bayesian.html#references",
    "title": "Global Warming Rate Estimation using Linear Gaussian Model",
    "section": "References",
    "text": "References\nAllen, M.R., O.P. Dube, W. Solecki, F. Aragón-Durand, W. Cramer, S. Humphreys, M. Kainuma, J. Kala, N. Mahowald, Y. Mulugetta, R. Perez, M. Wairiu, and K. Zickfeld, 2018: Framing and Context. In: Global Warming of 1.5°C. An IPCC Special Report on the impacts of global warming of 1.5°C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty [Masson-Delmotte, 15 V., P. Zhai, H.-O. Pörtner, D. Roberts, J. Skea, P.R. Shukla, A. Pirani, W. Moufouma-Okia, C. Péan, R. Pidcock, S. Connors, J.B.R. Matthews, Y. Chen, X. Zhou, M.I. Gomis, E. Lonnoy, T. Maycock, M. Tignor, and T. Waterfield (eds.)]. Cambridge University Press, Cambridge, UK and New York, NY, USA, pp. 49-92"
  }
]
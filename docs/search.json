[
  {
    "objectID": "convolution.html",
    "href": "convolution.html",
    "title": "Kernel convolutions",
    "section": "",
    "text": "library(jpeg)\nset.seed(123)\n\n\nLoading the image\nReading the image from a jpg file into a raster array. The image is loaded into an array of (500, 500, 3).\n\n# Reading the sample image\nimg <- readJPEG(\"assets/wall.jpg\", native = FALSE)\ndim(img)\n\n[1] 500 500   3\n\n\nPlotting the the raster image.\n\n\n\nplotting function\n\nplot_raster <- function(raster) {\n  par(mfrow = c(1,1), mar = c(1,1,1,1))\n  plot.new()\n  as.raster(raster) |>\n    rasterImage(xleft = 0, xright = 1, ytop = 0, ybottom = 1)\n}\n\n\n\nplot_raster(img)\n\n\n\n\nOriginal photo\n\n\n\n\n\n\nConvolving kernel\nThe convolution can be expressed as \\[\ng(x,y) = M * f(x,y) = \\sum_{dx=-a}^{a} \\sum_{dy=-b}^{b} M(dx, dy) f(x-dx, y-dy)\n\\]\nThat is, we calculate a new value for each pixel using the product of the convolution kernel and matrix of the same dimension from array f, in a rolling basis.\nHowever, the edge pixels might cause problems when using a large kernel, as there are no adjacent pixels. This can be handled by edge padding.\n\nExtended edge padding\nIn order to calculate convolutions also for the edge pixels, we can extend the edges by repeating edge values indefinitely.\n\n\n\n\n\nPadding edge pixels by extension (Michael 2013).\n\n\n\n\nThe following function repeats the edge pixels of a given matrix for a specified mount of times.\n\nedge_extension <- function(mat, pad) {\n    n <- nrow(mat)\n    m <- ncol(mat)\n\n    top   <- c(rep(mat[1,1], pad), mat[1,], rep(mat[1,m], pad))\n    bot   <- c(rep(mat[n,1], pad), mat[n,], rep(mat[n,m], pad))\n    left  <- matrix(rep(mat[1:n, 1], pad), ncol=pad)\n    right <- matrix(rep(mat[1:n, m], pad), ncol=pad)\n    mid   <- cbind(left, mat[1:n,], right)\n\n    new_mat <- rbind(\n        matrix(rep(top, pad), nrow=pad, byrow=TRUE),\n        mid,\n        matrix(rep(bot, pad), nrow=pad, byrow=TRUE)\n    )\n    \n    return(new_mat)\n}\n\nThe function in action when given a following 3x3 matrix as input \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\]\n\nedge_extension(matrix(1:9, ncol=3, byrow=T), pad=2) |>\n  write.table(row.names=F, col.names=F)\n\n1 1 1 2 3 3 3\n1 1 1 2 3 3 3\n1 1 1 2 3 3 3\n4 4 4 5 6 6 6\n7 7 7 8 9 9 9\n7 7 7 8 9 9 9\n7 7 7 8 9 9 9\n\n\n\n\nApplying the function\nThe following function calculates the rolling kernel using edge extension\n\nrolling_kernel <- function(f, M) {\n  \n  x <- ncol(f)\n  y <- nrow(f)\n  m <- ncol(M)\n  n <- nrow(M)\n  g <- f # g(x,y)\n  \n  # Kernel dimensions exceed the source\n  if (m > x ||  n  > y) {\n    stop(\"Kernel length exceeds the source\")\n  }\n  \n  g_extended <- edge_extension(f, (m-1))\n  \n  for (i in 1:x) {\n    for (j in 1:y) {\n      f_kernel <- g_extended[i:(i+(m-1)), j:(j+(n-1))]\n      g_kernel <- f_kernel * M\n      g[i, j] <- sum(g_kernel)\n    }\n  }\n\n  return(g)\n}\n\nA main wrapper function that convolves the given kernel for all three channels of an RGB-image. We have to also scale the values into [0,1] by computing \\(\\frac{\\mathrm{rank}(A_{m \\times n})}{m \\cdot n}\\)\n\nrgb_kernel <- function(raster, kernel) {\n  channels <- dim(raster)[3]\n  output <- raster\n\n  for (ch in 1:channels) {\n    output[,,ch] <- rolling_kernel(raster[,,ch], kernel)\n  }\n\n  # Scaling\n  output_reshape <- matrix(output, nrow = ncol(raster))\n  output_scaled  <- rank(output_reshape) / length(output_reshape)\n  dim(output_scaled) <- c(dim(raster))\n  \n  return(output_scaled)\n}\n\n\n\nTesting out common kernels\n\n# Sharpen\nimg |>\n  rgb_kernel(matrix(c(0,-1,0,-1,5,-1,0,-1,0), nrow=3)) |>\n  plot_raster()\n\n\n\n# Edge detection  \nimg |>\n  rgb_kernel(matrix(c(-1,-1,-1,-1,8,-1,-1,-1,-1), nrow=3)) |>\n  plot_raster()\n\n\n\n# Box blur\nimg |>\n  rgb_kernel(matrix(rep(1, 81), nrow=9)) |>\n  plot_raster()\n\n\n\n\n\n\n\n2-D Gaussian kernel\nGaussian blur can be applied by running a convolution using a Gaussian kernel. The two-dimensional Guaissian kernel is defined as \\[\nG(x,y) = \\frac{1}{2 \\pi \\sigma ^2} e^{- \\frac{x^2 + y^2}{2 \\sigma ^2}}\n\\]\nUsing the CDF of normal distribution, we can calculate the convolution matrix by\n\nGenerating a linspace \\(u = \\texttt{linspace}[\\texttt{a}, \\texttt{b}] \\in \\mathbb{R}^n\\)\nUsing \\(u\\) to draw values from the Gaussian \\(\\texttt{CDF}\\)\nCalculating the convolution matrix \\(M\\) by taking the outer product \\(M = uu^\\top \\in \\mathbb{R}^{n \\times n}\\)\n\n\n# Gaussian kernel function\ngaussian_kernel <- function(width, sigma = 1) {\n    if (width%%2 != 1) { stop(\"Even width parameter\") }\n\n    # Boundaries\n    b  <- (width-1)/2\n    a <- -b\n    \n    # Setting up the linspace\n    ax <- seq(a, b, length = width)\n\n    # Draw from gaussian CDF\n    u <- dnorm(ax, sd = sigma)\n\n    # Outer product uu^T and normalization\n    M <- u %o% u\n    M <- M / sum(M)\n\n    return(M)\n}\n\nPlotting an example convolution matrix using width = 81 and sigma = 10.\n\n\nCalculating color values for the plot\n# Modified from: https://stackoverflow.com/a/39118422\nz <- gaussian_kernel(width = 81, sigma = 10)\ncolors <- colorRampPalette(c(\"green\", \"yellow\", \"red\"))(100)\n\nz.facet.range <- ((z[-1, -1] +\n                   z[-1, -ncol(z)] +\n                   z[-nrow(z), -1] +\n                   z[-nrow(z), -ncol(z)]) / 4) |>\n                \n                cut(100)\n\n\n\n# Preparing the variables\nM <- gaussian_kernel(width = 81, sigma = 10)\nx <- y <- seq(-40, 40, length = 81)\n\n# Plotting the 3D surface\npar(mfrow = c(1,1), mar = c(1,1,1,1))\npersp(x, y, M,\n    theta = 135, phi = 30,\n    zlab = \"density\",\n    col = colors[z.facet.range]\n)\n\n\n\n\n3-D representation of the 2-D gaussian convolution\n\n\n\n\n\n\nGaussian blurring\nFinally, convolving the gaussian kernel and applying a Gaussian Blurof radius: 40 and sd: 10.\n\n# Gaussian blur\nimg |>\n  rgb_kernel(gaussian_kernel(width = 81, sigma = 10)) |>\n  plot_raster()\n\n\n\n\nOriginal image + gaussian blur\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMichael, Plotke. 2013. Image Kernel Convolution, Extend Edge-Handling. Wikipedia. https://en.wikipedia.org/wiki/File:Extend_Edge-Handling.png."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jieming You",
    "section": "",
    "text": "Latest projects\n\n\nSähkö Nyt\nElectricity price monitoring app for iOS developed using Swift and SwiftUI. Launching early 2023.\n\n\nSanajahti Solver\nDepth-first search algorithm implementation using Python for solving Sanajahti-game. Web-app built using Flask.\n\n\nGlobal Warming analysis\nBayesian stats project analysing the global warming trend using gaussian regressions. Markov Chain Monte Carlo simulations done in R and Stan.\n\n\nKernel Convolutions\nTesting kernel convolutions, edge extensions, and calculating a simple gaussian kernel using base R.\n\n\n\n\nUniversity projects\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nStrategy Project Strategy project & research conducted for Supermetrics. Research results and suggestions presented to the CEO.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nOperations Management Operations optimization for Finnair Kitchen. We mapped the the current processes and designed a new set of KPI for the management.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nMetro Optimization We aimed to increase Helsinki metro peak capacity by optimizing people flow using computer vision. Prototype presented to the HSL.\n\n\n\n\nOther stuff\n\n\nDeep Dive Case competition  1st place\nDeep tech Research-to-Business (R2B) case competition.\n\n\nOptiver TraderHack Hackathon  finalist\nAlgorithmic trading competition (Cross-Exchange Market Making)."
  },
  {
    "objectID": "sanajahti.html",
    "href": "sanajahti.html",
    "title": "Sanajahti Solver",
    "section": "",
    "text": "Site work in progress…"
  },
  {
    "objectID": "bayesian/bayesian.html",
    "href": "bayesian/bayesian.html",
    "title": "Predicting the rate of global warming",
    "section": "",
    "text": "There has been, and still is, a lot of controversy about the presence and magnitude of climate change. According to many studies, earth’s temperature has been steadily rising due to different factors, such as increased global emissions and deforestation. These studies propose differing values for the rate of the warming. Based on a report of IPCC, human-induced warming is likely increasing the average temperatures between 0.1°C and 0.3°C per decade.\nThe main analysis problem of this report is to find out the presence and the rate of global warming. We aim to model the global warming rate using Bayesian workflow. The analysis will be carried out using temperature observations gathered from 4 cities (Beijing, Helsinki, Los Angeles, Sydney) from a 25-year time span. We will fit three variants of the Gaussian linear model to average monthly temperatures of the four different locations. The goal is then to find an estimate for the slope of the linear model. As an example, the monthly data from Los Angeles is illustrated in Figure 1.\nFirst we will introduce the data, model, and priors that we are going to use. Then we will describe how we have run the model and conduct the convergence diagnostics, posterior predictive checks and sensitivity analysis. Lastly we will discuss the issues and improvements as well as final conclusions and self-reflection."
  }
]